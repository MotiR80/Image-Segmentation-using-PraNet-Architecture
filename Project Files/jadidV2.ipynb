{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Op8bnU6Lq2n"
      },
      "source": [
        "# **Download dataset and unzip it**\n",
        "----> **Run this code just one time!** <----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK7cUKYZ9V6c",
        "outputId": "47c88803-198c-4fba-9970-8897f5d6fb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://datasets.simula.no/downloads/kvasir-seg.zip\n",
        "!unzip kvasir-seg.zip"
      ],
      "metadata": {
        "id": "ZfWeTHKZ--bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_urqllBMZdRr",
        "outputId": "338b633b-3b18-4588-e86a-90fcb002245d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PRANet-Polyps-Segmentation'...\n",
            "remote: Enumerating objects: 549, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 549 (delta 6), reused 3 (delta 3), pack-reused 542\u001b[K\n",
            "Receiving objects: 100% (549/549), 114.05 MiB | 35.39 MiB/s, done.\n",
            "Resolving deltas: 100% (330/330), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Thehunk1206/PRANet-Polyps-Segmentation.git\n",
        "!cp -r /content/PRANet-Polyps-Segmentation/model  .\n",
        "!cp -r /content/PRANet-Polyps-Segmentation/utils  .\n",
        "!rm -rf /content/PRANet-Polyps-Segmentation\n",
        "!mkdir results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libaries**"
      ],
      "metadata": {
        "id": "yuT5A8TUBC4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "from utils.losses import WBCEDICELoss\n",
        "from utils.dataset import TfdataPipeline\n",
        "from model.PRA_net import PRAnet\n",
        "import tensorflow as tf\n",
        "from utils.segmentation_metric import dice_coef, iou_metric, MAE, WFbetaMetric, SMeasure, Emeasure\n",
        "from tensorflow.python.data.ops.dataset_ops import DatasetV2\n",
        "from tensorflow.keras import models\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from tensorflow.python.ops.image_ops_impl import ResizeMethod\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "metadata": {
        "id": "3fNvR_KlBJ5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIJIdGDNNZTU"
      },
      "source": [
        "# **Building Model And Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9nPARe1PE7c",
        "outputId": "fa9d85c9-88bc-4996-ade1-067d1632c5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 3s 0us/step\n",
            "Model: \"PRAnet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " resnet50_include_top_false  [(None, 64, 64, 256),        2358771   ['input_2[0][0]']             \n",
            "  (Functional)                (None, 32, 32, 512),        2                                       \n",
            "                              (None, 16, 16, 1024),                                               \n",
            "                              (None, 8, 8, 2048)]                                                 \n",
            "                                                                                                  \n",
            " rfb_4 (RFB)                 (None, 8, 8, 32)             392544    ['resnet50_include_top_false[0\n",
            "                                                                    ][3]']                        \n",
            "                                                                                                  \n",
            " rfb_3 (RFB)                 (None, 16, 16, 32)           228704    ['resnet50_include_top_false[0\n",
            "                                                                    ][2]']                        \n",
            "                                                                                                  \n",
            " rfb_2 (RFB)                 (None, 32, 32, 32)           146784    ['resnet50_include_top_false[0\n",
            "                                                                    ][1]']                        \n",
            "                                                                                                  \n",
            " partial_decoder (PartialDe  (None, 32, 32, 1)            278817    ['rfb_4[0][0]',               \n",
            " coder)                                                              'rfb_3[0][0]',               \n",
            "                                                                     'rfb_2[0][0]']               \n",
            "                                                                                                  \n",
            " resize4 (Resizing)          (None, 8, 8, 1)              0         ['partial_decoder[0][0]']     \n",
            "                                                                                                  \n",
            " reverse_attention_br4 (Rev  (None, 8, 8, 1)              5444869   ['resnet50_include_top_false[0\n",
            " erseAttention)                                                     ][3]',                        \n",
            "                                                                     'resize4[0][0]']             \n",
            "                                                                                                  \n",
            " resize3 (Resizing)          (None, 16, 16, 1)            0         ['reverse_attention_br4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " reverse_attention_br3 (Rev  (None, 16, 16, 1)            140805    ['resnet50_include_top_false[0\n",
            " erseAttention)                                                     ][2]',                        \n",
            "                                                                     'resize3[0][0]']             \n",
            "                                                                                                  \n",
            " resize2 (Resizing)          (None, 32, 32, 1)            0         ['reverse_attention_br3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " reverse_attention_br2 (Rev  (None, 32, 32, 1)            108037    ['resnet50_include_top_false[0\n",
            " erseAttention)                                                     ][1]',                        \n",
            "                                                                     'resize2[0][0]']             \n",
            "                                                                                                  \n",
            " salient_out_5 (Resizing)    (None, 256, 256, 1)          0         ['partial_decoder[0][0]']     \n",
            "                                                                                                  \n",
            " salient_out_4 (Resizing)    (None, 256, 256, 1)          0         ['reverse_attention_br4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " salient_out_3 (Resizing)    (None, 256, 256, 1)          0         ['reverse_attention_br3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " final_salient_out_2 (Resiz  (None, 256, 256, 1)          0         ['reverse_attention_br2[0][0]'\n",
            " ing)                                                               ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 30328272 (115.69 MB)\n",
            "Trainable params: 30268554 (115.47 MB)\n",
            "Non-trainable params: 59718 (233.27 KB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "==========Model configs==========\n",
            "Training and validating PRAnet for 25 epochs \n",
            "learing_rate: 0.001 \n",
            "Input shape:(256,256,3) \n",
            "Batch size: 8 \n",
            "Backbone arc: resnet50 \n",
            "Backbone Trainable: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [01:48<00:00,  1.08s/steps]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:08<00:00,  1.45steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:1.95 - epoch: 1 - loss: 2.3014204502105713 - dice: 0.7414169311523438 - IoU: 0.6186339259147644 - val_loss: 5.779956340789795 - val_dice: 3.3045694865492117e-19 - val_IoU: 0.0 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:14<00:00,  6.75steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.99steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 2 - loss: 1.682214379310608 - dice: 0.7942037582397461 - IoU: 0.6805530786514282 - val_loss: 6.431103706359863 - val_dice: 3.3045694865492117e-19 - val_IoU: 0.0 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:14<00:00,  6.72steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.81steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 3 - loss: 1.1947736740112305 - dice: 0.8730417490005493 - IoU: 0.7827832698822021 - val_loss: 6.684840202331543 - val_dice: 3.3045694865492117e-19 - val_IoU: 0.0 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:20<00:00,  4.88steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 20.20steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.35 - epoch: 4 - loss: 1.0903748273849487 - dice: 0.8991771936416626 - IoU: 0.8187495470046997 - val_loss: 6.828523635864258 - val_dice: 3.3045694865492117e-19 - val_IoU: 0.0 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:14<00:00,  6.67steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.65steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 5 - loss: 0.9097881317138672 - dice: 0.9150948524475098 - IoU: 0.8439108729362488 - val_loss: 7.607952117919922 - val_dice: 3.3045694865492117e-19 - val_IoU: 0.0 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model at /content/drive/MyDrive/trained_model/...\n",
            "model saved at /content/drive/MyDrive/trained_model/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.46steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 24.18steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.27 - epoch: 6 - loss: 1.1226420402526855 - dice: 0.8874132633209229 - IoU: 0.8008228540420532 - val_loss: 8.039094924926758 - val_dice: 3.3045694865492117e-19 - val_IoU: 0.0 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:14<00:00,  6.67steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.85steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 7 - loss: 0.962528645992279 - dice: 0.9104844331741333 - IoU: 0.8368620872497559 - val_loss: 6.889871597290039 - val_dice: 3.3045694865492117e-19 - val_IoU: 0.0 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:14<00:00,  6.67steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.55steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 8 - loss: 0.7985283136367798 - dice: 0.9258408546447754 - IoU: 0.8636079430580139 - val_loss: 5.030370712280273 - val_dice: 0.21117088198661804 - val_IoU: 0.17196881771087646 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:20<00:00,  4.89steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 21.13steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.35 - epoch: 9 - loss: 0.9285917282104492 - dice: 0.9176560640335083 - IoU: 0.8489720821380615 - val_loss: 3.555083751678467 - val_dice: 0.5403826832771301 - val_IoU: 0.4207243323326111 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.64steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.48steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 10 - loss: 0.9750457406044006 - dice: 0.9102895259857178 - IoU: 0.8404217958450317 - val_loss: 2.309605836868286 - val_dice: 0.6753308176994324 - val_IoU: 0.6168214082717896 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model at /content/drive/MyDrive/trained_model/...\n",
            "model saved at /content/drive/MyDrive/trained_model/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.58steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.67steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 11 - loss: 0.7543209791183472 - dice: 0.9283473491668701 - IoU: 0.867800235748291 - val_loss: 1.5406849384307861 - val_dice: 0.8341243267059326 - val_IoU: 0.7482954263687134 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.64steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.08steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 12 - loss: 0.6943879127502441 - dice: 0.9375870227813721 - IoU: 0.8831126689910889 - val_loss: 1.5587424039840698 - val_dice: 0.8188931941986084 - val_IoU: 0.7396512031555176 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:20<00:00,  4.89steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 20.85steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.35 - epoch: 13 - loss: 0.6155073642730713 - dice: 0.9447202682495117 - IoU: 0.8955438137054443 - val_loss: 1.6553741693496704 - val_dice: 0.7905910015106201 - val_IoU: 0.6742314100265503 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.62steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.20steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 14 - loss: 0.6066497564315796 - dice: 0.9469993114471436 - IoU: 0.8996536135673523 - val_loss: 1.8086917400360107 - val_dice: 0.8421146273612976 - val_IoU: 0.7384793758392334 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.59steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.08steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 15 - loss: 0.566040575504303 - dice: 0.9533501863479614 - IoU: 0.9110949039459229 - val_loss: 1.790685772895813 - val_dice: 0.8135406970977783 - val_IoU: 0.7212449312210083 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model at /content/drive/MyDrive/trained_model/...\n",
            "model saved at /content/drive/MyDrive/trained_model/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.58steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.48steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 16 - loss: 0.5662965774536133 - dice: 0.9509921073913574 - IoU: 0.9068719744682312 - val_loss: 1.307141661643982 - val_dice: 0.8455063700675964 - val_IoU: 0.7430897355079651 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.59steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.30steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 17 - loss: 0.546355664730072 - dice: 0.9552727341651917 - IoU: 0.9147423505783081 - val_loss: 1.8528587818145752 - val_dice: 0.8191965818405151 - val_IoU: 0.714682400226593 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.63steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.70steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 18 - loss: 0.5117009878158569 - dice: 0.9602827429771423 - IoU: 0.9239192008972168 - val_loss: 1.1892342567443848 - val_dice: 0.8684498071670532 - val_IoU: 0.788508415222168 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.58steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.27steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 19 - loss: 0.4557315707206726 - dice: 0.9663265943527222 - IoU: 0.9349252581596375 - val_loss: 1.0484495162963867 - val_dice: 0.8760969638824463 - val_IoU: 0.8056252002716064 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.60steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.07steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 20 - loss: 0.42226889729499817 - dice: 0.9702930450439453 - IoU: 0.9424219131469727 - val_loss: 1.036953091621399 - val_dice: 0.8837442994117737 - val_IoU: 0.8137525320053101 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model at /content/drive/MyDrive/trained_model/...\n",
            "model saved at /content/drive/MyDrive/trained_model/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.59steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.45steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 21 - loss: 0.40440744161605835 - dice: 0.9708181619644165 - IoU: 0.9433499574661255 - val_loss: 1.1434648036956787 - val_dice: 0.8760643005371094 - val_IoU: 0.7996811270713806 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.61steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 20.88steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 22 - loss: 0.3928317129611969 - dice: 0.9729300737380981 - IoU: 0.9473636150360107 - val_loss: 1.0129520893096924 - val_dice: 0.8865821361541748 - val_IoU: 0.8120559453964233 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.60steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.39steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 23 - loss: 0.41227203607559204 - dice: 0.9648968577384949 - IoU: 0.9323859214782715 - val_loss: 1.16818368434906 - val_dice: 0.872435450553894 - val_IoU: 0.7921082973480225 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.61steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.34steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 24 - loss: 0.38075557351112366 - dice: 0.9719101786613464 - IoU: 0.9454149007797241 - val_loss: 1.0940110683441162 - val_dice: 0.8732898235321045 - val_IoU: 0.790854275226593 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training...: 100%|\u001b[31m██████████\u001b[0m| 100/100 [00:15<00:00,  6.56steps/s]\n",
            "Validating...: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 23.24steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETA:0.26 - epoch: 25 - loss: 0.44279852509498596 - dice: 0.9673148989677429 - IoU: 0.9368215203285217 - val_loss: 1.2860968112945557 - val_dice: 0.8664425611495972 - val_IoU: 0.7902510166168213 \n",
            "\n",
            "Writing to Tensorboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model at /content/drive/MyDrive/trained_model/...\n",
            "model saved at /content/drive/MyDrive/trained_model/\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(41)\n",
        "\n",
        "\n",
        "def process_output(x: tf.Tensor, threshold:float = None):\n",
        "\n",
        "\n",
        "    x = tf.sigmoid(x)\n",
        "    if threshold:\n",
        "        x = tf.cast(tf.math.greater(x, threshold), tf.float32)\n",
        "    x = x * 255.0\n",
        "    return x\n",
        "\n",
        "\n",
        "def train(\n",
        "    dataset_dir: str,\n",
        "    trained_model_dir: str,\n",
        "    img_size: int = 352,\n",
        "    batch_size: int = 8,\n",
        "    epochs: int = 25,\n",
        "    lr: float = 1e-3,\n",
        "    gclip: float = 1.0,\n",
        "    dataset_split: float = 0.1,\n",
        "    backbone_trainable: bool = True,\n",
        "    backbone_arc:str = 'resnet50',\n",
        "    logdir: str = \"logs/\",\n",
        "):\n",
        "    assert os.path.isdir(dataset_dir)\n",
        "    if backbone_arc == 'mobilenetv2' and img_size > 224:\n",
        "        tf.print(f\"For backbone {backbone_arc} inputsize should be 32 < inputsize <=224\")\n",
        "        sys.exit()\n",
        "\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"No dir named {dataset_dir} exist\")\n",
        "        sys.exit()\n",
        "\n",
        "    if not os.path.exists(trained_model_dir):\n",
        "        os.mkdir(path=trained_model_dir)\n",
        "\n",
        "    # instantiate tf.summary writer\n",
        "    logsdir = logdir + \"PRAnet/\" + \"PRAnet_\"+backbone_arc+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_writer = tf.summary.create_file_writer(logsdir + \"/train/\")\n",
        "    val_writer = tf.summary.create_file_writer(logsdir + \"/val/\")\n",
        "\n",
        "    # initialize tf.data pipeline\n",
        "    tf_datapipeline = TfdataPipeline(\n",
        "        BASE_DATASET_DIR=dataset_dir,\n",
        "        IMG_H=img_size,\n",
        "        IMG_W=img_size,\n",
        "        batch_size=batch_size,\n",
        "        split=dataset_split\n",
        "    )\n",
        "    train_data = tf_datapipeline.data_loader(dataset_type='train')\n",
        "    val_data = tf_datapipeline.data_loader(dataset_type='valid')\n",
        "\n",
        "    # instantiate optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=lr,\n",
        "    )\n",
        "\n",
        "    # instantiate loss function\n",
        "    loss_fn = WBCEDICELoss(name='w_bce_dice_loss')\n",
        "\n",
        "\n",
        "    # instantiate model (PRAnet)\n",
        "    pranet = PRAnet(\n",
        "        IMG_H=img_size,\n",
        "        IMG_W=img_size,\n",
        "        filters=32,\n",
        "        backbone_arch=backbone_arc,\n",
        "        backbone_trainable=backbone_trainable\n",
        "    )\n",
        "\n",
        "    # compile the model\n",
        "    pranet.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss_fn,\n",
        "    )\n",
        "    tf.print(pranet.build_graph(inshape=(img_size, img_size, 3)).summary())\n",
        "    tf.print(\"==========Model configs==========\")\n",
        "    tf.print(\n",
        "        f\"Training and validating PRAnet for {epochs} epochs \\nlearing_rate: {lr} \\nInput shape:({img_size},{img_size},3) \\nBatch size: {batch_size} \\nBackbone arc: {backbone_arc} \\nBackbone Trainable: {backbone_trainable}\"\n",
        "    )\n",
        "    # train for epochs\n",
        "    for e in range(epochs):\n",
        "        t = time()\n",
        "\n",
        "        for (x_train_img, y_train_mask) in tqdm(train_data, unit='steps', desc='training...', colour='red'):\n",
        "            train_loss, train_dice, train_iou = pranet.train_step(\n",
        "                x_img=x_train_img, y_mask=y_train_mask, gclip=gclip)\n",
        "\n",
        "        for (x_val_img, y_val_mask) in tqdm(val_data, unit='steps', desc='Validating...', colour='green'):\n",
        "            val_loss, val_dice, val_iou = pranet.test_step(x_img=x_val_img, y_mask=y_val_mask)\n",
        "\n",
        "        tf.print(\n",
        "            \"ETA:{} - epoch: {} - loss: {} - dice: {} - IoU: {} - val_loss: {} - val_dice: {} - val_IoU: {} \\n\".format(\n",
        "                round((time() - t)/60, 2), (e+1), train_loss, train_dice, train_iou, val_loss, val_dice, val_iou)\n",
        "            )\n",
        "\n",
        "\n",
        "        tf.print(\"Writing to Tensorboard...\")\n",
        "        lateral_out_sg, lateral_out_s4, lateral_out_s3, lateral_out_s2 = pranet(x_val_img, training=False)\n",
        "        lateral_out_sg = process_output(lateral_out_sg)\n",
        "        lateral_out_s4 = process_output(lateral_out_s4)\n",
        "        lateral_out_s3 = process_output(lateral_out_s3)\n",
        "        lateral_out_s2 = process_output(lateral_out_s2, threshold = 0.3)\n",
        "\n",
        "\n",
        "        with train_writer.as_default():\n",
        "            tf.summary.scalar(name='train_loss', data=train_loss, step=e+1)\n",
        "            tf.summary.scalar(name='dice', data = train_dice, step=e+1)\n",
        "            tf.summary.scalar(name='iou', data = train_iou, step=e+1)\n",
        "\n",
        "\n",
        "        with val_writer.as_default():\n",
        "            tf.summary.scalar(name='val_loss', data=val_loss, step=e+1)\n",
        "            tf.summary.scalar(name='val_dice', data=val_dice, step=e+1)\n",
        "            tf.summary.scalar(name='val_dice', data=val_iou, step=e+1)\n",
        "            tf.summary.image(name='Y_mask', data=y_val_mask*255, step=e+1, max_outputs=batch_size, description='Val data')\n",
        "            tf.summary.image(name='Global S Map', data=lateral_out_sg, step=e+1, max_outputs=batch_size, description='Val data')\n",
        "            tf.summary.image(name='S4 Map', data=lateral_out_s4, step=e+1, max_outputs=batch_size, description='Val data')\n",
        "            tf.summary.image(name='S3 Map', data=lateral_out_s3, step=e+1, max_outputs=batch_size, description='Val data')\n",
        "            tf.summary.image(name='S2 Map', data=lateral_out_s2, step=e+1, max_outputs=batch_size, description='Val data')\n",
        "\n",
        "        if (e+1)%5 == 0:\n",
        "            tf.print(\n",
        "                f\"Saving model at {trained_model_dir}...\"\n",
        "            )\n",
        "            pranet.save(f\"{trained_model_dir}pranet_{backbone_arc}\", save_format='tf')\n",
        "            tf.print(f\"model saved at {trained_model_dir}\")\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  if __name__ == \"__main__\":\n",
        "      train(\n",
        "          dataset_dir=\"Kvasir-SEG/\",\n",
        "          trained_model_dir=\"/content/drive/MyDrive/trained_model/\",\n",
        "          img_size=256,\n",
        "          batch_size=8,\n",
        "          epochs=25\n",
        "\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CerHXW1GPFhe"
      },
      "source": [
        "# **Prediction**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLquLsMCPJcc",
        "outputId": "124f2444-b586-430b-b2d1-e08ac5a7dd85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] loading model from disk....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_546460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model <keras.src.saving.legacy.saved_model.load.PRAnet object at 0x7c15c6e4c4c0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing..: 100%|\u001b[32m██████████\u001b[0m| 100/100 [00:20<00:00,  4.88steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average runtime of model: 29.08628865979382ms \n",
            " Mean IoU: 0.7655985951423645\n",
            " Mean Dice coef: 0.8483993411064148\n",
            " Mean wfb: 0.8481339812278748\n",
            " Mean Smeasure: 0.8494685888290405\n",
            " Mean Emeasure: 0.9085527062416077\n",
            " MAE: 0.0558440200984478\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_model(model_path: str):\n",
        "    assert isinstance(model_path, str)\n",
        "\n",
        "    tf.print(\n",
        "        \"[info] loading model from disk....\"\n",
        "    )\n",
        "    model = models.load_model(model_path)\n",
        "\n",
        "    tf.print(\n",
        "        \"loaded model {}\".format(model)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def datapipeline(dataset_path: str, imgsize: int = 352) -> DatasetV2:\n",
        "    assert isinstance(dataset_path, str)\n",
        "\n",
        "    tfpipeline = TfdataPipeline(\n",
        "        BASE_DATASET_DIR=dataset_path, IMG_H=imgsize, IMG_W=imgsize, batch_size=1)\n",
        "    test_data = tfpipeline.data_loader(dataset_type='test')\n",
        "\n",
        "    return test_data\n",
        "\n",
        "\n",
        "def run_test(\n",
        "    model_path: str,\n",
        "    imgsize: int = 352,\n",
        "    dataset_path: str = 'polyps_dataset/',\n",
        "    threshold: float = 0.5\n",
        "):\n",
        "    assert os.path.exists(model_path)\n",
        "    assert os.path.exists(dataset_path)\n",
        "    assert 1.0 > threshold > 0.0\n",
        "\n",
        "    pranet = get_model(model_path=model_path)\n",
        "    test_data = datapipeline(dataset_path=dataset_path, imgsize=imgsize)\n",
        "\n",
        "    # initialize metrics\n",
        "    wfb_metric = WFbetaMetric()\n",
        "    smeasure_metric = SMeasure()\n",
        "    emeasure_metric = Emeasure()\n",
        "    # collect metric for individual test data to average it later\n",
        "    dice_coefs = []\n",
        "    ious = []\n",
        "    wfbs = []\n",
        "    smeasures = []\n",
        "    emeasures = []\n",
        "    maes = []\n",
        "    runtimes = []\n",
        "\n",
        "    for (image, mask) in tqdm(test_data, desc='Testing..', unit='steps', colour='green'):\n",
        "        start = time()\n",
        "        outs = pranet(image)\n",
        "        end = time()\n",
        "        # squesh the out put between 0-1\n",
        "        final_out = tf.sigmoid(outs[-1])\n",
        "        # convert the out map to binary map\n",
        "        final_out = tf.cast(tf.math.greater(final_out, 0.5), tf.float32)\n",
        "\n",
        "        total_time = round((end - start)*1000, ndigits=2)\n",
        "\n",
        "        dice = dice_coef(y_mask=mask, y_pred=final_out)\n",
        "        iou = iou_metric(y_mask=mask, y_pred=final_out)\n",
        "        mae = MAE(y_mask=mask, y_pred= final_out)\n",
        "        wfb = wfb_metric(y_mask=mask, y_pred=final_out)\n",
        "        smeasure = smeasure_metric(y_mask=mask, y_pred=final_out)\n",
        "        emeasure = emeasure_metric(y_mask=mask, y_pred= final_out)\n",
        "\n",
        "        dice_coefs.append(dice)\n",
        "        ious.append(iou)\n",
        "        maes.append(mae)\n",
        "        wfbs.append(wfb)\n",
        "        smeasures.append(smeasure)\n",
        "        emeasures.append(emeasure)\n",
        "        runtimes.append(total_time)\n",
        "\n",
        "    mean_dice = sum(dice_coefs)/len(dice_coefs)\n",
        "    mean_iou = sum(ious)/len(ious)\n",
        "    mean_mae = sum(maes)/len(maes)\n",
        "    mean_wfb = sum(wfbs)/len(wfbs)\n",
        "    mean_smeasure = sum(smeasures)/len(smeasures)\n",
        "    mean_emeasure = sum(emeasures)/len(emeasures)\n",
        "    mean_runtime = sum(runtimes[3:]) / len(runtimes[3:])\n",
        "    tf.print(\n",
        "        f\"Average runtime of model: {mean_runtime}ms \\n\",\n",
        "        f\"Mean IoU: {mean_iou}\\n\",\n",
        "        f\"Mean Dice coef: {mean_dice}\\n\",\n",
        "        f\"Mean wfb: {mean_wfb}\\n\",\n",
        "        f\"Mean Smeasure: {mean_smeasure}\\n\",\n",
        "        f\"Mean Emeasure: {mean_emeasure}\\n\",\n",
        "        f\"MAE: {mean_mae}\\n\",\n",
        "    )\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  if __name__ == \"__main__\":\n",
        "      run_test(\n",
        "          model_path='/content/drive/MyDrive/trained_model/pranet_resnet50',\n",
        "          dataset_path='Kvasir-SEG/',\n",
        "          imgsize=256\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2TKrtILbNLZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Results**"
      ],
      "metadata": {
        "id": "YcA8YMSo_gM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFyh2X7rbNeQ"
      },
      "outputs": [],
      "source": [
        "def read_image(path: str, img_size: int = 352) -> tf.Tensor:\n",
        "    image_raw = tf.io.read_file(path)\n",
        "    original_image = tf.io.decode_jpeg(image_raw, channels=3)\n",
        "    original_image = tf.cast(original_image, dtype=tf.float32)\n",
        "    original_image = original_image/255.0\n",
        "\n",
        "    resized_image = tf.image.resize(original_image, [img_size, img_size])\n",
        "    resized_image = tf.expand_dims(resized_image, axis=0)\n",
        "\n",
        "    return resized_image, original_image\n",
        "\n",
        "def read_mask(path: str, img_size: int = 352) -> tf.Tensor:\n",
        "    image_raw = tf.io.read_file(path)\n",
        "    mask = tf.io.decode_jpeg(image_raw, channels=1)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def process_output(x: tf.Tensor, original_img: tf.Tensor, threshold: float = None) -> tf.Tensor:\n",
        "    x = tf.sigmoid(x)\n",
        "\n",
        "    if threshold:\n",
        "        x = tf.cast(tf.math.greater(x, threshold), tf.float32)\n",
        "\n",
        "    x = tf.squeeze(x, axis=0)\n",
        "    x = tf.image.resize(x, [original_img.shape[0], original_img.shape[1]], ResizeMethod.BICUBIC)\n",
        "    # we use tf.tile to make multiple copy of output single channel image\n",
        "    mutiple_const = tf.constant([1,1,3]) # [1,1,3] h(1)xw(1)xc(3)\n",
        "    x = tf.tile(x,mutiple_const)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_model(model_path: str):\n",
        "    assert isinstance(model_path, str)\n",
        "\n",
        "    tf.print(\n",
        "        \"[info] loading model from disk....\"\n",
        "    )\n",
        "    model = models.load_model(model_path)\n",
        "\n",
        "    tf.print(\n",
        "        \"[info] loaded model\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def select_random_images(directory, percentage=0.1):\n",
        "    if not os.path.exists(directory) or not os.path.isdir(directory):\n",
        "        raise ValueError(\"The provided path is not valid or is not a directory.\")\n",
        "\n",
        "    images_dir = os.path.join(directory, 'images')\n",
        "    masks_dir = os.path.join(directory, 'masks')\n",
        "\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    num_images_to_select = int(len(image_files) * percentage)\n",
        "\n",
        "    if num_images_to_select == 0:\n",
        "        raise ValueError(\"The number of images to select is zero.\")\n",
        "\n",
        "    selected_images = random.sample(image_files, num_images_to_select)\n",
        "    images_paths = [os.path.join(images_dir, img) for img in selected_images]\n",
        "    masks_paths = [os.path.join(masks_dir, img) for img in selected_images]\n",
        "\n",
        "    return images_paths, masks_paths\n",
        "\n",
        "def vis_predicted_mask(*images: tf.Tensor):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    grid_spec = gridspec.GridSpec(2, 3, width_ratios=[3, 3, 3])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(images[0])\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    plt.imshow(images[5], 'gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"True mask\")\n",
        "\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(images[1])\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Predicted Mask\")\n",
        "\n",
        "    plt.subplot(grid_spec[3])\n",
        "    plt.imshow(images[2])\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Global S Map\")\n",
        "\n",
        "    plt.subplot(grid_spec[4])\n",
        "    plt.imshow(images[3])\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Side Map 4\")\n",
        "\n",
        "    plt.subplot(grid_spec[5])\n",
        "    plt.imshow(images[4])\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Side Map 3\")\n",
        "\n",
        "    plt.grid('off')\n",
        "    plt.savefig(f\"results/detection_{time()}.jpg\")\n",
        "\n",
        "\n",
        "def run(\n",
        "    model_path: str,\n",
        "    dataset_path: str,\n",
        "    imgsize: int = 352,\n",
        "    threshold: float = 0.5\n",
        "):\n",
        "    assert os.path.exists(model_path)\n",
        "    assert os.path.exists(dataset_path)\n",
        "    assert 1.0 > threshold > 0.0\n",
        "\n",
        "    pranet = get_model(model_path=model_path)\n",
        "\n",
        "    images_path, masks_path = select_random_images(dataset_path, .05)\n",
        "\n",
        "    for image_path, mask_path in zip(images_path, masks_path):\n",
        "      input_image, original_image = read_image(\n",
        "          path=image_path, img_size=imgsize)\n",
        "\n",
        "      true_mask = read_mask(path=mask_path, img_size=imgsize)\n",
        "\n",
        "      tf.print(\"[info] Computing output mask..\")\n",
        "      start = time()\n",
        "      outs = pranet(input_image)\n",
        "      end = time()\n",
        "      sg, s4, s3, final_out = outs\n",
        "      final_out = process_output(final_out, original_img=original_image, threshold=threshold)\n",
        "      sg = process_output(sg, original_img=original_image)\n",
        "      s4 = process_output(s4, original_img=original_image)\n",
        "      s3 = process_output(s3, original_img=original_image)\n",
        "\n",
        "\n",
        "      total_time = round((end - start)*1000, ndigits=2)\n",
        "      tf.print(f\"Total runtime of model: {total_time}ms\")\n",
        "\n",
        "\n",
        "      vis_predicted_mask(original_image, final_out, sg, s4, s3, true_mask)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  if __name__ == \"__main__\":\n",
        "    run(\n",
        "        model_path='/content/drive/MyDrive/trained_model/pranet_resnet50',\n",
        "        dataset_path='/content/Kvasir-SEG',\n",
        "        imgsize=256\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download The Results**"
      ],
      "metadata": {
        "id": "RnAmNwXbFlFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip results.zip results/*\n",
        "files.download('results.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Tn-27lzaC5aM",
        "outputId": "8a9e3a9f-4af2-4a2c-93d3-de6c69cdbbdf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7f0a7a9e-0cca-4b8d-b0ef-24c429984cb2\", \"results.zip\", 1898933)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}